{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b551cf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drcjar/anaconda3/envs/dlessons/lib/python3.6/site-packages/pdfminer/pdfdocument.py:8: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography. The next release of cryptography will remove support for Python 3.6.\n",
      "  from cryptography.hazmat.backends import default_backend\n"
     ]
    }
   ],
   "source": [
    "# üêç Standard library\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import urllib.request\n",
    "\n",
    "# üåê Third-party packages\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import pdfplumber\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "\n",
    "def fetch_all_pfd_pdf_links(base_url=\"https://www.judiciary.uk/page/{}/?s&pfd_report_type&post_type=pfd&order=date\",\n",
    "                             delay=1.0,\n",
    "                             verbose=True,\n",
    "                             cache_file=\"cached_pdf_links.json\"):\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, \"r\") as f:\n",
    "            cached_links = json.load(f)\n",
    "            if verbose:\n",
    "                print(f\"[CACHE] Loaded {len(cached_links)} PDF links from {cache_file}\")\n",
    "            return cached_links\n",
    "\n",
    "    page_num = 0\n",
    "    people_urls = []\n",
    "\n",
    "    while True:\n",
    "        url = base_url.format(page_num)\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 404:\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        links = [a['href'] for a in soup.find_all('a', class_='card__link') if a.has_attr('href')]\n",
    "        people_urls.extend(links)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[Page {page_num}] Fetched {len(links)} links\")\n",
    "\n",
    "        page_num += 1\n",
    "        time.sleep(delay)\n",
    "\n",
    "    pdf_urls = []\n",
    "    for report_url in people_urls:\n",
    "        try:\n",
    "            print(f\"[FETCHING PDF LINK] {report_url}\")\n",
    "            res = requests.get(report_url, timeout=15)\n",
    "            soup = BeautifulSoup(res.content, 'html.parser')\n",
    "            links = soup.find_all('a')\n",
    "            found_pdf = False\n",
    "\n",
    "            for link in links:\n",
    "                href = link.get('href')\n",
    "                if href and href.endswith('.pdf'):\n",
    "                    pdf_urls.append(href)\n",
    "                    found_pdf = True\n",
    "\n",
    "            if not found_pdf:\n",
    "                print(f\"[WARNING] No PDF found on page: {report_url}\")\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"[TIMEOUT] Skipped {report_url} due to timeout.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to fetch PDF from {report_url}: {e}\")\n",
    "\n",
    "        time.sleep(delay)\n",
    "\n",
    "    pdf_urls = list(set(pdf_urls))  # deduplicate\n",
    "    pd.DataFrame({'urls': people_urls}).drop_duplicates().to_csv(\"urls.csv\", index=False)\n",
    "    pd.DataFrame({'pdf_urls': pdf_urls}).drop_duplicates().to_csv(\"pdf_urls.csv\", index=False)\n",
    "\n",
    "    with open(cache_file, \"w\") as f:\n",
    "        json.dump(pdf_urls, f, indent=2)\n",
    "        if verbose:\n",
    "            print(f\"[CACHE] Saved {len(pdf_urls)} PDF links to {cache_file}\")\n",
    "\n",
    "    return pdf_urls\n",
    "\n",
    "\n",
    "def download_pdfs(pdf_urls, download_dir=\"downloads\"):\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    for link in pdf_urls:\n",
    "        link = link.strip()\n",
    "        filename = os.path.basename(link)\n",
    "        filepath = os.path.join(download_dir, filename)\n",
    "        if os.path.isfile(filepath):\n",
    "            print(f\"[SKIP] {filename} already exists\")\n",
    "            continue\n",
    "        try:\n",
    "            print(f\"[DOWNLOADING] {filename}\")\n",
    "            urllib.request.urlretrieve(link, filepath)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to download {link}: {e}\")\n",
    "\n",
    "def extract_text_smart(pdf_path, min_length=200, min_space_ratio=0.05):\n",
    "    def space_ratio(text):\n",
    "        if not text:\n",
    "            return 0\n",
    "        return text.count(\" \") / len(text)\n",
    "\n",
    "    # Step 1: Try pdfminer\n",
    "    try:\n",
    "        resource_manager = PDFResourceManager()\n",
    "        fake_file_handle = io.StringIO()\n",
    "        converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n",
    "        page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "\n",
    "        with open(pdf_path, 'rb') as fh:\n",
    "            for page in PDFPage.get_pages(fh, caching=True, check_extractable=False):\n",
    "                page_interpreter.process_page(page)\n",
    "            text = fake_file_handle.getvalue()\n",
    "\n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "\n",
    "        if text and len(text.strip()) >= min_length:\n",
    "            ratio = space_ratio(text)\n",
    "            if ratio >= min_space_ratio:\n",
    "                return text, 'pdfminer'\n",
    "            else:\n",
    "                print(f\"[LOW SPACE RATIO: {ratio:.3f}] Falling back to pdfplumber\")\n",
    "    except Exception as e:\n",
    "        print(f\"[PDFMINER ERROR] {e}\")\n",
    "\n",
    "    # Step 2: Try pdfplumber\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            text = \"\\n\".join([page.extract_text() or \"\" for page in pdf.pages])\n",
    "        if text and len(text.strip()) >= min_length:\n",
    "            return text, 'pdfplumber'\n",
    "    except Exception as e:\n",
    "        print(f\"[PDFPLUMBER ERROR] {e}\")\n",
    "\n",
    "    return \"\", \"ocr\"\n",
    "\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "def extract_text_from_pdfs(download_dir=\"downloads\", text_dir=\"texts\",\n",
    "                           log_path=\"extraction_source.json\"):\n",
    "    os.makedirs(text_dir, exist_ok=True)\n",
    "\n",
    "    # Load existing extraction log\n",
    "    if os.path.exists(log_path):\n",
    "        with open(log_path, \"r\") as f:\n",
    "            source_log = json.load(f)\n",
    "    else:\n",
    "        source_log = {}\n",
    "\n",
    "    for filepath in glob.glob(os.path.join(download_dir, '*.pdf')):\n",
    "        filename = os.path.basename(filepath)\n",
    "        output_path = os.path.join(text_dir, filename + '.txt')\n",
    "\n",
    "        # Skip if already processed\n",
    "        if filename in source_log and os.path.exists(output_path):\n",
    "            print(f\"[SKIP] Already extracted ({source_log[filename].upper()}): {filename}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(f\"[EXTRACTING] {filename}\")\n",
    "            text, source = extract_text_smart(filepath)\n",
    "\n",
    "            if text:\n",
    "                with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(text)\n",
    "                source_log[filename] = source\n",
    "                print(f\"[SAVED] from {source.upper()}\")\n",
    "            else:\n",
    "                print(f\"[NEEDS OCR] {filename}\")\n",
    "                source_log[filename] = \"ocr\"\n",
    "\n",
    "            # üíæ Save after every file so progress is retained\n",
    "            with open(log_path, \"w\") as f:\n",
    "                json.dump(source_log, f, indent=2)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to extract {filename}: {e}\")\n",
    "\n",
    "\n",
    "    # Save updated log  \n",
    "    source_log[filename] = source\n",
    "    with open(log_path, \"w\") as f:\n",
    "        json.dump(source_log, f, indent=2)\n",
    " \n",
    "\n",
    "    print(f\"\\nüìò Updated extraction log saved to {log_path}\")\n",
    "\n",
    "           \n",
    "def extract_ocr_text_for_missing(text_dir=\"texts\", download_dir=\"downloads\",\n",
    "                                 extraction_log=\"extraction_source.json\"):\n",
    "    if not os.path.exists(extraction_log):\n",
    "        print(\"[INFO] No extraction source log found. OCR step skipped.\")\n",
    "        return\n",
    "\n",
    "    with open(extraction_log, \"r\") as f:\n",
    "        extraction_source = json.load(f)\n",
    "\n",
    "    for pdf_file, method in extraction_source.items():\n",
    "        if method != \"ocr\":\n",
    "            continue\n",
    "\n",
    "        pdf_path = os.path.join(download_dir, pdf_file)\n",
    "        txt_path = os.path.join(text_dir, pdf_file + \".txt\")\n",
    "\n",
    "        # Skip if already has decent text\n",
    "        if os.path.exists(txt_path) and os.path.getsize(txt_path) > 50:\n",
    "            print(f\"[SKIP] OCR already done for {pdf_file}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(f\"[OCR] Running OCR for {pdf_file}\")\n",
    "            images = convert_from_path(pdf_path)\n",
    "\n",
    "            ocr_text_chunks = []\n",
    "            for i, img in enumerate(images):\n",
    "                chunk = pytesseract.image_to_string(img)\n",
    "                ocr_text_chunks.append(chunk)\n",
    "                if i == 0:  # only print first page for debugging\n",
    "                    print(f\"[OCR DEBUG] Page 1 text preview:\\n{chunk.strip()[:300]}\\n\")\n",
    "\n",
    "            full_text = \"\\n\".join(ocr_text_chunks)\n",
    "\n",
    "            if full_text.strip():\n",
    "                with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(full_text)\n",
    "                print(f\"[‚úÖ OCR SAVED] {txt_path}\")\n",
    "            else:\n",
    "                print(f\"[‚ùå OCR TEXT EMPTY] {pdf_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] OCR failed for {pdf_file}: {e}\")\n",
    "            \n",
    "def extract_metadata_from_urls(url_csv=\"urls.csv\", pickle_path=\"people_data.pickle\", json_path=\"people_data.json\"):\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    import os\n",
    "    import json\n",
    "    import pickle\n",
    "\n",
    "    if os.path.exists(pickle_path):\n",
    "        with open(pickle_path, 'rb') as f:\n",
    "            people = pickle.load(f)\n",
    "    else:\n",
    "        people = {}\n",
    "\n",
    "    df = pd.read_csv(url_csv)\n",
    "    url_col = df.columns[0]\n",
    "    urls = list(df[url_col])\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            page = requests.get(url, timeout=15)\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "            # Find PDF link to extract filename (used as the key)\n",
    "            pdf_link = next((a['href'] for a in soup.find_all('a') if a.get('href', '').endswith('.pdf')), None)\n",
    "            if not pdf_link:\n",
    "                print(f\"[WARNING] No PDF link found at {url}\")\n",
    "                continue\n",
    "\n",
    "            filename = os.path.basename(pdf_link)\n",
    "            if filename in people:\n",
    "                continue\n",
    "\n",
    "            # Extract metadata paragraphs\n",
    "            paragraphs = [str(p) for p in soup.find_all('p')]\n",
    "\n",
    "            people[filename] = paragraphs\n",
    "            print(f\"[SCRAPED] {filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to get metadata from {url}: {e}\")\n",
    "\n",
    "    with open(pickle_path, 'wb') as f:\n",
    "        pickle.dump(people, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(people, f, indent=2)\n",
    "\n",
    "    return people\n",
    "\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def combine_text_and_metadata_to_json(\n",
    "    text_dir=\"texts\",\n",
    "    metadata_path=\"people_data.json\",\n",
    "    output_path=\"data.json\",\n",
    "    extraction_log=\"extraction_source.json\"\n",
    "):\n",
    "    def clean(text):\n",
    "        return re.sub(r\"[\\xa0\\u200b\\n\\r]+\", \" \", text).strip()\n",
    "\n",
    "    def extract_fields(paragraphs):\n",
    "        fields = {\n",
    "            \"date_of_report\": \"\",\n",
    "            \"ref\": \"\",\n",
    "            \"name_of_deceased\": \"\",\n",
    "            \"coroner_name\": \"\",\n",
    "            \"coroner_area\": \"\",\n",
    "            \"category\": \"\",\n",
    "        }\n",
    "\n",
    "        for html in paragraphs:\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            text = clean(soup.get_text())\n",
    "            text_lower = text.lower()\n",
    "\n",
    "            if \"date of report\" in text_lower:\n",
    "                fields[\"date_of_report\"] = text\n",
    "            elif text_lower.startswith(\"ref\"):\n",
    "                fields[\"ref\"] = text.replace(\"Ref:\", \"\").strip()\n",
    "            elif \"deceased name\" in text_lower:\n",
    "                fields[\"name_of_deceased\"] = text\n",
    "            elif \"coroners name\" in text_lower:\n",
    "                fields[\"coroner_name\"] = text\n",
    "            elif \"coroners area\" in text_lower:\n",
    "                fields[\"coroner_area\"] = text\n",
    "            elif \"category\" in text_lower:\n",
    "                fields[\"category\"] = text\n",
    "\n",
    "        return fields\n",
    "\n",
    "    with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "        metadata_dict = json.load(f)\n",
    "\n",
    "    if os.path.exists(extraction_log):\n",
    "        with open(extraction_log, 'r') as f:\n",
    "            extraction_source = json.load(f)\n",
    "    else:\n",
    "        extraction_source = {}\n",
    "\n",
    "    data = []\n",
    "    for text_file in glob.glob(os.path.join(text_dir, '*.txt')):\n",
    "        filename = os.path.basename(text_file).replace('.txt', '')\n",
    "        pdf_filename = filename.replace('.pdf', '') + '.pdf'\n",
    "\n",
    "        with open(text_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        raw_meta = metadata_dict.get(pdf_filename, [])\n",
    "        fields = extract_fields(raw_meta)\n",
    "        year_match = re.search(r\"\\b(20\\d{2})\\b\", fields.get(\"date_of_report\", \"\"))\n",
    "        year = int(year_match.group(1)) if year_match else \"\"\n",
    "\n",
    "        base_slug = os.path.splitext(pdf_filename)[0].lower().replace(\" \", \"-\")\n",
    "        clean_slug = re.sub(r'-\\d{4}-\\d{4}(_.*)?$', '', base_slug)\n",
    "        entry = {\n",
    "            \"person\": base_slug,\n",
    "            **fields,\n",
    "            \"filename\": filename,\n",
    "            \"text\": content,\n",
    "            \"url\": f\"https://www.judiciary.uk/prevention-of-future-death-reports/{clean_slug}/\",\n",
    "            \"year_of_report\": year,\n",
    "            \"source\": extraction_source.get(pdf_filename, \"unknown\")\n",
    "}\n",
    "        entry.pop(\"source\", None)  # üî• Strip out 'source' for Tantivy\n",
    "        entry.pop(\"year_of_report\", None)# üî• Strip out 'year_of_report' for Tantivy\n",
    "        data.append(entry)\n",
    "        \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for entry in data:\n",
    "            f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f\"[DONE] Combined {len(data)} entries into {output_path}\")\n",
    "\n",
    "from datetime import datetime\n",
    "import time as t\n",
    "\n",
    "def run_full_pipeline(checkpoint_file=\"pipeline_checkpoint.json\"):\n",
    "    stages = [\n",
    "        \"fetch_pdf_links\",\n",
    "        \"download_pdfs\",\n",
    "        \"extract_text\",\n",
    "        \"extract_ocr\",\n",
    "        \"extract_metadata\",\n",
    "        \"combine_json\"\n",
    "    ]\n",
    "\n",
    "    # Load checkpoint if it exists\n",
    "    checkpoint = {}\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, \"r\") as f:\n",
    "            checkpoint = json.load(f)\n",
    "\n",
    "    def mark_done(stage):\n",
    "        checkpoint[\"stage\"] = stage\n",
    "        checkpoint[\"last_completed\"] = datetime.now().isoformat()\n",
    "        with open(checkpoint_file, \"w\") as f:\n",
    "            json.dump(checkpoint, f, indent=2)\n",
    "\n",
    "    def should_run(stage):\n",
    "        return checkpoint.get(\"stage\") is None or stages.index(checkpoint[\"stage\"]) < stages.index(stage)\n",
    "\n",
    "    # Stage 1: Fetch PDF links\n",
    "    if should_run(\"fetch_pdf_links\"):\n",
    "        print(\"üîé Fetching PDF links...\")\n",
    "        start = t.time()\n",
    "        pdf_urls = fetch_all_pfd_pdf_links()\n",
    "        print(f\"‚úÖ Done in {t.time() - start:.1f}s\")\n",
    "        mark_done(\"fetch_pdf_links\")\n",
    "    else:\n",
    "        with open(\"cached_pdf_links.json\") as f:\n",
    "            pdf_urls = json.load(f)\n",
    "        print(\"‚úÖ Skipping PDF link fetch (already done)\")\n",
    "\n",
    "    # Stage 2: Download PDFs\n",
    "    if should_run(\"download_pdfs\"):\n",
    "        print(\"\\n‚¨áÔ∏è Downloading PDFs...\")\n",
    "        start = t.time()\n",
    "        download_pdfs(pdf_urls)\n",
    "        print(f\"‚úÖ Done in {t.time() - start:.1f}s\")\n",
    "        mark_done(\"download_pdfs\")\n",
    "    else:\n",
    "        print(\"‚úÖ Skipping PDF download (already done)\")\n",
    "\n",
    "    # Stage 3: Extract text\n",
    "    if should_run(\"extract_text\"):\n",
    "        print(\"\\nüìÑ Extracting text from PDFs...\")\n",
    "        start = t.time()\n",
    "        extract_text_from_pdfs()\n",
    "        print(f\"‚úÖ Done in {t.time() - start:.1f}s\")\n",
    "        mark_done(\"extract_text\")\n",
    "    else:\n",
    "        print(\"‚úÖ Skipping text extraction (already done)\")\n",
    "\n",
    "    # Stage 4: Run OCR\n",
    "    if should_run(\"extract_ocr\"):\n",
    "        print(\"\\nüß† Running OCR on PDFs that need it...\")\n",
    "        start = t.time()\n",
    "        extract_ocr_text_for_missing()\n",
    "        print(f\"‚úÖ Done in {t.time() - start:.1f}s\")\n",
    "        mark_done(\"extract_ocr\")\n",
    "    else:\n",
    "        print(\"‚úÖ Skipping OCR (already done)\")\n",
    "\n",
    "    # Stage 5: Extract metadata\n",
    "    if should_run(\"extract_metadata\"):\n",
    "        print(\"\\nüóÇÔ∏è Extracting metadata from report pages...\")\n",
    "        start = t.time()\n",
    "        extract_metadata_from_urls()\n",
    "        print(f\"‚úÖ Done in {t.time() - start:.1f}s\")\n",
    "        mark_done(\"extract_metadata\")\n",
    "    else:\n",
    "        print(\"‚úÖ Skipping metadata extraction (already done)\")\n",
    "\n",
    "    # Stage 6: Combine into JSON\n",
    "    if should_run(\"combine_json\"):\n",
    "        print(\"\\nüì¶ Combining text and metadata into JSON...\")\n",
    "        start = t.time()\n",
    "        combine_text_and_metadata_to_json()\n",
    "        print(f\"‚úÖ Done in {t.time() - start:.1f}s\")\n",
    "        mark_done(\"combine_json\")\n",
    "    else:\n",
    "        print(\"‚úÖ Skipping final JSON build (already done)\")\n",
    "\n",
    "    print(\"\\nüèÅ Pipeline complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22298534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_extraction_prefers_pdfplumber (__main__.TestPdfPlumberFallback) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EXTRACTING] Thomas-Ratchford-2018-0147_Redacted.pdf\n",
      "[LOW SPACE RATIO: 0.000] Falling back to pdfplumber\n",
      "[SAVED] from PDFPLUMBER\n",
      "\n",
      "üìò Updated extraction log saved to test_pdfplumber/extraction_source.json\n",
      "[DONE] Combined 1 entries into test_pdfplumber/data.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FAIL\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_extraction_prefers_pdfplumber (__main__.TestPdfPlumberFallback)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-d2678b3ba447>\", line 52, in test_extraction_prefers_pdfplumber\n",
      "    self.assertEqual(len(data), 1)\n",
      "AssertionError: 10 != 1\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.589s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7fdfb0fdc1d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test case need pdfplumber\n",
    "\n",
    "import unittest\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "class TestPdfPlumberFallback(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.test_dir = \"test_pdfplumber\"\n",
    "        self.download_dir = os.path.join(self.test_dir, \"downloads\")\n",
    "        self.text_dir = os.path.join(self.test_dir, \"texts\")\n",
    "        self.meta_json = os.path.join(self.test_dir, \"people_data.json\")\n",
    "        self.output_json = os.path.join(self.test_dir, \"data.json\")\n",
    "        self.extract_log = os.path.join(self.test_dir, \"extraction_source.json\")\n",
    "\n",
    "        os.makedirs(self.download_dir, exist_ok=True)\n",
    "        os.makedirs(self.text_dir, exist_ok=True)\n",
    "\n",
    "        # ‚úÖ Use your known-good local file\n",
    "        self.filename = \"Thomas-Ratchford-2018-0147_Redacted.pdf\"\n",
    "        self.local_pdf_path = os.path.join(\"downloads\", self.filename)\n",
    "\n",
    "        assert os.path.exists(self.local_pdf_path), \"Missing local test PDF in downloads/\"\n",
    "        shutil.copy(self.local_pdf_path, os.path.join(self.download_dir, self.filename))\n",
    "\n",
    "        # ‚úÖ Create minimal dummy metadata\n",
    "        dummy_slug = \"Thomas-Ratchford-2018-0147_Redacted\"\n",
    "        dummy_data = {dummy_slug: [\"<p>Dummy metadata paragraph</p>\"]}\n",
    "        with open(self.meta_json, \"w\") as f:\n",
    "            json.dump(dummy_data, f, indent=2)\n",
    "\n",
    "    def test_extraction_prefers_pdfplumber(self):\n",
    "        extract_text_from_pdfs(download_dir=self.download_dir,\n",
    "                               text_dir=self.text_dir,\n",
    "                               log_path=self.extract_log)\n",
    "\n",
    "        with open(self.extract_log) as f:\n",
    "            log = json.load(f)\n",
    "\n",
    "        self.assertIn(self.filename, log)\n",
    "        self.assertEqual(log[self.filename], \"pdfplumber\", \"Expected fallback to pdfplumber\")\n",
    "\n",
    "        combine_text_and_metadata_to_json(text_dir=self.text_dir,\n",
    "                                          metadata_path=self.meta_json,\n",
    "                                          output_path=self.output_json,\n",
    "                                          extraction_log=self.extract_log)\n",
    "\n",
    "        with open(self.output_json) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.assertEqual(len(data), 1)\n",
    "        self.assertIn(\"text\", data[0])\n",
    "        self.assertIn(\"metadata\", data[0])\n",
    "        self.assertEqual(data[0][\"source\"], \"pdfplumber\")\n",
    "        self.assertGreater(len(data[0][\"text\"]), 100)\n",
    "\n",
    "        # ‚úÖ Show a preview of the extracted text\n",
    "        print(\"\\n‚úÖ Extracted text preview:\\n\")\n",
    "        print(data[0][\"text\"][:500])  # Show first 500 characters of the extracted text\n",
    "\n",
    "    def tearDown(self):\n",
    "        shutil.rmtree(self.test_dir)\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a953db44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_extraction_uses_pdfminer (__main__.TestPdfMinerPreferred) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EXTRACTING] 2014-0061-Response-from-Care-UK.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FAIL\n",
      "test_extraction_prefers_pdfplumber (__main__.TestPdfPlumberFallback) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVED] from PDFMINER\n",
      "\n",
      "üìò Updated extraction log saved to test_pdfminer/extraction_source.json\n",
      "[DONE] Combined 1 entries into test_pdfminer/data.json\n",
      "[EXTRACTING] Thomas-Ratchford-2018-0147_Redacted.pdf\n",
      "[LOW SPACE RATIO: 0.000] Falling back to pdfplumber\n",
      "[SAVED] from PDFPLUMBER\n",
      "\n",
      "üìò Updated extraction log saved to test_pdfplumber/extraction_source.json\n",
      "[DONE] Combined 1 entries into test_pdfplumber/data.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FAIL\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_extraction_uses_pdfminer (__main__.TestPdfMinerPreferred)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-3-f0b9fdb8f872>\", line 50, in test_extraction_uses_pdfminer\n",
      "    self.assertEqual(len(data), 1)\n",
      "AssertionError: 10 != 1\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_extraction_prefers_pdfplumber (__main__.TestPdfPlumberFallback)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-d2678b3ba447>\", line 52, in test_extraction_prefers_pdfplumber\n",
      "    self.assertEqual(len(data), 1)\n",
      "AssertionError: 10 != 1\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.786s\n",
      "\n",
      "FAILED (failures=2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7fdfb0fdc160>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test case need ocr\n",
    "\n",
    "import unittest\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "class TestPdfMinerPreferred(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.test_dir = \"test_pdfminer\"\n",
    "        self.download_dir = os.path.join(self.test_dir, \"downloads\")\n",
    "        self.text_dir = os.path.join(self.test_dir, \"texts\")\n",
    "        self.meta_json = os.path.join(self.test_dir, \"people_data.json\")\n",
    "        self.output_json = os.path.join(self.test_dir, \"data.json\")\n",
    "        self.extract_log = os.path.join(self.test_dir, \"extraction_source.json\")\n",
    "\n",
    "        os.makedirs(self.download_dir, exist_ok=True)\n",
    "        os.makedirs(self.text_dir, exist_ok=True)\n",
    "\n",
    "        self.filename = \"2014-0061-Response-from-Care-UK.pdf\"\n",
    "        self.local_pdf_path = os.path.join(\"downloads\", self.filename)\n",
    "\n",
    "        assert os.path.exists(self.local_pdf_path), \"Missing test PDF for pdfminer in downloads/\"\n",
    "        shutil.copy(self.local_pdf_path, os.path.join(self.download_dir, self.filename))\n",
    "\n",
    "        dummy_slug = \"2014-0061-Response-from-Care-UK\"\n",
    "        dummy_data = {dummy_slug: [\"<p>Dummy metadata</p>\"]}\n",
    "        with open(self.meta_json, \"w\") as f:\n",
    "            json.dump(dummy_data, f, indent=2)\n",
    "\n",
    "    def test_extraction_uses_pdfminer(self):\n",
    "        extract_text_from_pdfs(download_dir=self.download_dir,\n",
    "                               text_dir=self.text_dir,\n",
    "                               log_path=self.extract_log)\n",
    "\n",
    "        with open(self.extract_log) as f:\n",
    "            log = json.load(f)\n",
    "\n",
    "        self.assertIn(self.filename, log)\n",
    "        self.assertEqual(log[self.filename], \"pdfminer\")\n",
    "\n",
    "        combine_text_and_metadata_to_json(text_dir=self.text_dir,\n",
    "                                          metadata_path=self.meta_json,\n",
    "                                          output_path=self.output_json,\n",
    "                                          extraction_log=self.extract_log)\n",
    "\n",
    "        with open(self.output_json) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.assertEqual(len(data), 1)\n",
    "        self.assertIn(\"text\", data[0])\n",
    "        self.assertIn(\"metadata\", data[0])\n",
    "        self.assertEqual(data[0][\"source\"], \"pdfminer\")\n",
    "        self.assertGreater(len(data[0][\"text\"]), 100)\n",
    "\n",
    "    def tearDown(self):\n",
    "        shutil.rmtree(self.test_dir)\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8a095be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Skipping PDF link fetch (already done)\n",
      "‚úÖ Skipping PDF download (already done)\n",
      "‚úÖ Skipping text extraction (already done)\n",
      "‚úÖ Skipping OCR (already done)\n",
      "‚úÖ Skipping metadata extraction (already done)\n",
      "‚úÖ Skipping final JSON build (already done)\n",
      "\n",
      "üèÅ Pipeline complete.\n"
     ]
    }
   ],
   "source": [
    "run_full_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69aa6edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need a urls.csv containing a single reference\n",
    "\n",
    "def run_test_pipeline_single(url_csv=\"urls.csv\", test_dir=\"test_run\"):\n",
    "    import shutil\n",
    "\n",
    "    # Create a test directory\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    # Clean test outputs inside test_dir\n",
    "    for f in [\"cached_pdf_links.json\", \"people_data.json\", \"people_data.pickle\",\n",
    "              \"extraction_source.json\", \"data.json\"]:\n",
    "        path = os.path.join(test_dir, f)\n",
    "        if os.path.exists(path):\n",
    "            os.remove(path)\n",
    "    shutil.rmtree(os.path.join(test_dir, \"downloads\"), ignore_errors=True)\n",
    "    shutil.rmtree(os.path.join(test_dir, \"texts\"), ignore_errors=True)\n",
    "\n",
    "    # Fetch PDF link from the single report URL\n",
    "    print(\"\\nüîó Fetching PDF URL from page...\")\n",
    "    df = pd.read_csv(url_csv)\n",
    "    url = df.iloc[0, 0]\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    pdf_links = [a[\"href\"] for a in soup.find_all(\"a\") if a.get(\"href\", \"\").endswith(\".pdf\")]\n",
    "    if not pdf_links:\n",
    "        print(\"‚ùå No PDF link found.\")\n",
    "        return\n",
    "    pdf_url = pdf_links[0]\n",
    "    print(f\"‚úÖ Found PDF: {pdf_url}\")\n",
    "\n",
    "    # Save cached_pdf_links.json manually\n",
    "    with open(os.path.join(test_dir, \"cached_pdf_links.json\"), \"w\") as f:\n",
    "        json.dump([pdf_url], f)\n",
    "\n",
    "    # Functions redirected to test_dir\n",
    "    print(\"\\n‚¨áÔ∏è Downloading...\")\n",
    "    download_pdfs([pdf_url], download_dir=os.path.join(test_dir, \"downloads\"))\n",
    "\n",
    "    print(\"\\nüìÑ Extracting text...\")\n",
    "    extract_text_from_pdfs(download_dir=os.path.join(test_dir, \"downloads\"),\n",
    "                           text_dir=os.path.join(test_dir, \"texts\"),\n",
    "                           log_path=os.path.join(test_dir, \"extraction_source.json\"))\n",
    "\n",
    "    print(\"\\nüß† Running OCR if needed...\")\n",
    "    extract_ocr_text_for_missing(text_dir=os.path.join(test_dir, \"texts\"),\n",
    "                                 download_dir=os.path.join(test_dir, \"downloads\"),\n",
    "                                 extraction_log=os.path.join(test_dir, \"extraction_source.json\"))\n",
    "\n",
    "    print(\"\\nüóÇÔ∏è Extracting metadata...\")\n",
    "    extract_metadata_from_urls(url_csv=url_csv,\n",
    "                               pickle_path=os.path.join(test_dir, \"people_data.pickle\"),\n",
    "                               json_path=os.path.join(test_dir, \"people_data.json\"))\n",
    "\n",
    "    print(\"\\nüì¶ Building final data.json...\")\n",
    "    combine_text_and_metadata_to_json(text_dir=os.path.join(test_dir, \"texts\"),\n",
    "                                      metadata_path=os.path.join(test_dir, \"people_data.json\"),\n",
    "                                      output_path=os.path.join(test_dir, \"data.json\"),\n",
    "                                      extraction_log=os.path.join(test_dir, \"extraction_source.json\"))\n",
    "\n",
    "    print(\"\\nüîç Preview of data.json:\")\n",
    "    with open(os.path.join(test_dir, \"data.json\")) as f:\n",
    "        one = json.loads(f.readline())\n",
    "        for k in one:\n",
    "            print(f\"{k}: {str(one[k])[:100]}{'...' if len(str(one[k])) > 100 else ''}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "945e8330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_test_pipeline_single(\"./test_run/urls.csv\") breaks metadata?? prob not, should test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dce538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in ref where it's missing\n",
    "\n",
    "# Load newline-delimited JSON file (each line is a separate JSON object)\n",
    "df = pd.read_json(\"data.json\", lines=True)\n",
    "\n",
    "# Treat empty strings as missing\n",
    "df['ref'].replace(\"\", pd.NA, inplace=True)\n",
    "\n",
    "# Fill 'ref' from the start of 'filename' where it's missing\n",
    "missing_ref = df['ref'].isna()\n",
    "df.loc[missing_ref, 'ref'] = df.loc[missing_ref, 'filename'].str.extract(r\"^(\\d{4}-\\d{4})\", expand=False)\n",
    "\n",
    "# Fill any remaining missing 'ref' with the 'url' since not all cases appear to have a ref\n",
    "df['ref'].fillna(df['url'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f07b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have issues with broken urls and missing refs still"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "630b1054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_url(url):\n",
    "    try:\n",
    "        if \"judiciary.uk\" in url:\n",
    "            r = requests.get(url, allow_redirects=True, timeout=5)\n",
    "            content = r.text.lower()\n",
    "            broken = (\n",
    "                r.status_code != 200 or\n",
    "                \"page not found\" in content or\n",
    "                \"sorry, we can‚Äôt find\" in content or\n",
    "                \"no results found\" in content\n",
    "            )\n",
    "            return url, not broken\n",
    "        else:\n",
    "            r = requests.head(url, allow_redirects=True, timeout=5)\n",
    "            return url, r.status_code == 200\n",
    "    except requests.RequestException:\n",
    "        return url, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "137df290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case\n",
    "def test_judiciary_soft_404():\n",
    "    url = \"https://www.judiciary.uk/prevention-of-future-death-reports/2016-0368-barnsley-hospital-nhs-trust/\"\n",
    "    checked_url, ok = check_url(url)\n",
    "    if ok is False:\n",
    "        print(\"‚úÖ Test passed: URL correctly marked as broken.\")\n",
    "    else:\n",
    "        print(\"‚ùå Test failed: URL incorrectly marked as working.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eafd2090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test passed: URL correctly marked as broken.\n"
     ]
    }
   ],
   "source": [
    "test_judiciary_soft_404()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5af2af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most complete row happens to have correct url (I think), this is v hacky\n",
    "\n",
    "# Define a completeness score that excludes nulls and empty strings/whitespace\n",
    "def completeness_score(row):\n",
    "    return sum(x not in [None, np.nan] and str(x).strip() != '' for x in row)\n",
    "\n",
    "# Apply completeness score\n",
    "df['completeness'] = df.apply(completeness_score, axis=1)\n",
    "\n",
    "# Get the most complete row per ref (that has a non-null URL)\n",
    "best_urls = (\n",
    "    df.dropna(subset=['ref', 'url'])\n",
    "      .sort_values('completeness', ascending=False)\n",
    "      .drop_duplicates('ref')[['ref', 'url']]\n",
    ")\n",
    "\n",
    "# Build mapping and update\n",
    "ref_to_best_url = best_urls.set_index('ref')['url']\n",
    "df['url'] = df['ref'].map(ref_to_best_url).fillna(df['url'])\n",
    "\n",
    "# Clean up\n",
    "df.drop(columns='completeness', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77e92e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get non-empty name_of_deceased for each ref\n",
    "ref_to_name = (\n",
    "    df[df['name_of_deceased'].notna() & (df['name_of_deceased'].str.strip() != '')]\n",
    "    .drop_duplicates('ref')\n",
    "    .set_index('ref')['name_of_deceased']\n",
    ")\n",
    "\n",
    "# Step 2: Fill missing or empty name_of_deceased using that mapping\n",
    "def fill_name(row):\n",
    "    if pd.isna(row['name_of_deceased']) or str(row['name_of_deceased']).strip() == '':\n",
    "        return ref_to_name.get(row['ref'], row['name_of_deceased'])\n",
    "    return row['name_of_deceased']\n",
    "\n",
    "df['name_of_deceased'] = df.apply(fill_name, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f13d6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking URLs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5440/5440 [05:32<00:00, 16.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked 5440 new URLs (cached: 8162).\n",
      "Found 0 broken URLs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Load or create cache\n",
    "cache_file = \"url_status_cache.json\"\n",
    "if os.path.exists(cache_file):\n",
    "    with open(cache_file, \"r\") as f:\n",
    "        url_cache = json.load(f)\n",
    "else:\n",
    "    url_cache = {}\n",
    "\n",
    "# Optional: force rechecking some URLs\n",
    "force_recheck = True  # set to True to ignore cache\n",
    "\n",
    "# Function to check a single URL\n",
    "def check_url(url):\n",
    "    try:\n",
    "        if \"judiciary.uk\" in url:\n",
    "            r = requests.get(url, allow_redirects=True, timeout=5)\n",
    "            content = r.text.lower()\n",
    "            broken = (\n",
    "                r.status_code != 200 or\n",
    "                \"page not found\" in content or\n",
    "                \"sorry, we can‚Äôt find\" in content or\n",
    "                \"no results found\" in content\n",
    "            )\n",
    "            return url, not broken\n",
    "        else:\n",
    "            r = requests.head(url, allow_redirects=True, timeout=5)\n",
    "            return url, r.status_code == 200\n",
    "    except requests.RequestException:\n",
    "        return url, False\n",
    "\n",
    "# Get URLs to check (exclude cached unless forced)\n",
    "urls_to_check = [url for url in df['url'].dropna().unique() if force_recheck or url not in url_cache]\n",
    "\n",
    "# Multithreaded URL checking with progress\n",
    "with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    futures = {executor.submit(check_url, url): url for url in urls_to_check}\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Checking URLs\"):\n",
    "        url, ok = future.result()\n",
    "        url_cache[url] = ok\n",
    "\n",
    "# Persist updated cache\n",
    "with open(cache_file, \"w\") as f:\n",
    "    json.dump(url_cache, f)\n",
    "\n",
    "# Map results back to DataFrame\n",
    "df['url_ok'] = df['url'].map(url_cache)\n",
    "\n",
    "# Filter broken URLs\n",
    "broken_urls = df[df['url_ok'] == False]\n",
    "\n",
    "# Output summary\n",
    "print(f\"Checked {len(urls_to_check)} new URLs (cached: {len(url_cache) - len(urls_to_check)}).\")\n",
    "print(f\"Found {len(broken_urls)} broken URLs.\")\n",
    "\n",
    "# Replace bad URLs using ref\n",
    "df['url_ok'] = df['url_ok'].fillna(False).astype(bool)\n",
    "ref_to_good_url = (\n",
    "    df[df['url_ok']]\n",
    "    .dropna(subset=['ref'])\n",
    "    .drop_duplicates('ref')\n",
    "    .set_index('ref')['url']\n",
    ")\n",
    "# ref_to_good_url = df[df['url_ok']].dropna(subset=['ref']).drop_duplicates('ref').set_index('ref')['url']\n",
    "df['url'] = df.apply(\n",
    "    lambda row: ref_to_good_url.get(row['ref'], row['url']) if not row.get('url_ok', True) else row['url'],\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d02fcf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try setting url appropriately in another way..\n",
    "\n",
    "mask = df['url_ok'] == False\n",
    "\n",
    "df.loc[mask, 'url'] = (\n",
    "    df.loc[mask, 'name_of_deceased']\n",
    "      .str.replace(r'^Deceased name:\\s*', '', regex=True)\n",
    "      .str.strip()\n",
    "      .str.lower()\n",
    "      .str.replace(r'[^a-z0-9]+', '-', regex=True)\n",
    "      .str.strip('-')\n",
    "      .apply(lambda name: f\"https://www.judiciary.uk/prevention-of-future-death-reports/{name}/\" if pd.notna(name) and name != '' else np.nan)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c38bb6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_to_search_url(name):\n",
    "    if not isinstance(name, str) or not name.strip().lower().startswith(\"deceased name:\"):\n",
    "        return np.nan\n",
    "    clean_name = name.replace(\"Deceased name:\", \"\").strip()\n",
    "    query = '+'.join(part.capitalize() for part in clean_name.split())\n",
    "    return f\"https://www.judiciary.uk/?s={query}\"\n",
    "\n",
    "# Apply only to broken URLs\n",
    "mask = df['url_ok'] == False\n",
    "df.loc[mask, 'url'] = df.loc[mask, 'name_of_deceased'].apply(name_to_search_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d8999f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I found a mistake on the internet https://www.judiciary.uk/prevention-of-future-death-reports/williams-vickers/\n",
    "# the 'broken' urls appear to be typos (based on checking the first three); strictly we ought to have captured\n",
    "# the slug urls during our download process for the situation when it does not meet the expected pattern\n",
    "# for now we accept this\n",
    "# an elgant alternative is actually to revert to a search of the pfd website\n",
    "# e.g. https://www.judiciary.uk/?s=FirstName+SecondName so lets do that\n",
    "# transforms = {'https://www.judiciary.uk/prevention-of-future-death-reports/william-vickers/':'https://www.judiciary.uk/prevention-of-future-death-reports/williams-vickers/', \n",
    "#             'https://www.judiciary.uk/prevention-of-future-death-reports/grenfell-tower/':'https://www.judiciary.uk/prevention-of-future-death-reports/2018-0262-prevention-of-future-deaths-report/', \n",
    "#             'https://www.judiciary.uk/prevention-of-future-death-reports/colin-sluman/':'https://www.judiciary.uk/prevention-of-future-death-reports/colin-james/'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca960065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9,  72,  77,  11,  81,  73,  86,  83,  80,  79,  84,  85,  98,\n",
       "       136,  65,  97,  71,  78,  88,  87, 117, 109,  76, 116,  74,  94,\n",
       "        10,  82,  95, 121, 105,  75,  67,   8,  89, 126, 100, 137,  69,\n",
       "        92, 106,  90,  96, 110, 114,  68,  99,  91, 131,  66,  93, 104,\n",
       "       124, 122, 154, 153, 112, 128, 127, 113, 118,  70, 111, 101, 108,\n",
       "       149, 132, 129, 107, 171, 115, 142,  35, 135, 130,  41, 267, 245,\n",
       "       281, 125])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ref'].str.len().unique() # some of our refs are not refs but are urls, I likely did that as some were missing\n",
    "# should review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc2a06df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5375"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.name_of_deceased.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "407b49dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Deceased name: Malyun Karama', 'Deceased name: Theresa Robertson',\n",
       "       'Deceased name: George Townsend', ...,\n",
       "       'Deceased name: Susan Williams', 'Deceased name: Keith Weston',\n",
       "       'Deceased name: Barbara Mitchell'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.name_of_deceased.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b4d555d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['malyun-karama-2020-0162_redacted',\n",
       "       'theresa-robertson-2020-0158_redacted',\n",
       "       'george-townsend-2020-0157_redacted', ...,\n",
       "       'john-jennings-2020-0257',\n",
       "       'sean-ennis-prevention-of-future-deaths-report-2022-0054',\n",
       "       'barbara-mitchell-prevention-of-future-deaths-report-2023-0153_published'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.person.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "514b66b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.name_of_deceased = df.name_of_deceased.str.replace(r'^Deceased name:\\s*', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee468abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person</th>\n",
       "      <th>date_of_report</th>\n",
       "      <th>ref</th>\n",
       "      <th>name_of_deceased</th>\n",
       "      <th>coroner_name</th>\n",
       "      <th>coroner_area</th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>url_ok</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>malyun-karama-2020-0162_redacted</td>\n",
       "      <td>Date of report: 21 August 2020</td>\n",
       "      <td>2020-0162</td>\n",
       "      <td>5375</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Category: Hospital death (Clinical procedure a...</td>\n",
       "      <td>Malyun-Karama-2020-0162_Redacted.pdf</td>\n",
       "      <td>\\n\\n \\n\\n \\n1 \\n\\n \\n2 \\n\\n \\n3 \\n\\n \\n4 \\n\\n...</td>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>theresa-robertson-2020-0158_redacted</td>\n",
       "      <td>Date of report: 6 August 2020</td>\n",
       "      <td>2020-0158</td>\n",
       "      <td>5375</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Category: Alcohol, drug and medication related...</td>\n",
       "      <td>Theresa-Robertson-2020-0158_Redacted.pdf</td>\n",
       "      <td>Regulation 28:  Prevention of Future Deaths re...</td>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>george-townsend-2020-0157_redacted</td>\n",
       "      <td>Date of report: 4 June 2020</td>\n",
       "      <td>2020-0157</td>\n",
       "      <td>5375</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Category: Community healthcare related deaths,...</td>\n",
       "      <td>George-Townsend-2020-0157_Redacted.pdf</td>\n",
       "      <td>REGULATION 28:  REPORT TO PREVENT FUTURE DEATH...</td>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jerrelle-mckenzie-2020-0144_redacted</td>\n",
       "      <td>Date of report: 17 July 2020</td>\n",
       "      <td>2020-0144</td>\n",
       "      <td>5375</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Category: Alcohol, drug and medication related...</td>\n",
       "      <td>Jerrelle-McKenzie-2020-0144_Redacted.pdf</td>\n",
       "      <td>47812-2019 \\n\\nSenior Coroner - Emma Whitting ...</td>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>joan-williams-2020-0128_redacted</td>\n",
       "      <td>Date of report: 16 June 2020</td>\n",
       "      <td>2020-0128</td>\n",
       "      <td>5375</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Category: Road (Highways Safety) related deaths</td>\n",
       "      <td>Joan-Williams-2020-0128_Redacted.pdf</td>\n",
       "      <td>48060-2019\\n\\nSenior Coroner - Emma Whitting\\n...</td>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11971</th>\n",
       "      <td>2014-0061-response-from-care-uk</td>\n",
       "      <td></td>\n",
       "      <td>2014-0061</td>\n",
       "      <td>5375</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2014-0061-Response-from-Care-UK.pdf</td>\n",
       "      <td>care  \\n\\nCare UK Clinical Services \\nLimited ...</td>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11972</th>\n",
       "      <td>neville-bardoliwalla-2020-0258</td>\n",
       "      <td>Date of report: 26 November 2020</td>\n",
       "      <td>2020-0258</td>\n",
       "      <td>5375</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Category: Suicide; Other related deaths</td>\n",
       "      <td>Neville-Bardoliwalla-2020-0258.pdf</td>\n",
       "      <td>\\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n ...</td>\n",
       "      <td>https://www.judiciary.uk/?s=Neville+Bardoliwalla</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11973</th>\n",
       "      <td>john-jennings-2020-0257</td>\n",
       "      <td>Date of report: 26 November 2020</td>\n",
       "      <td>2020-0257</td>\n",
       "      <td>5375</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Category: Emergency services related deaths; O...</td>\n",
       "      <td>John-Jennings-2020-0257.pdf</td>\n",
       "      <td>\\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n ...</td>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11974</th>\n",
       "      <td>sean-ennis-prevention-of-future-deaths-report-...</td>\n",
       "      <td>Date of report: 21 February 2022</td>\n",
       "      <td>2022-0054</td>\n",
       "      <td>5375</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Category: Other related deaths</td>\n",
       "      <td>Sean-Ennis-Prevention-of-future-deaths-report-...</td>\n",
       "      <td>Her Majesty‚Äôs Coroner for the \\nNorthern Distr...</td>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11975</th>\n",
       "      <td>barbara-mitchell-prevention-of-future-deaths-r...</td>\n",
       "      <td>Date of report: 12/05/2023</td>\n",
       "      <td>2023-0153</td>\n",
       "      <td>5375</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Category: Care Home Health related deathsThis ...</td>\n",
       "      <td>Barbara-Mitchell-Prevention-of-future-deaths-r...</td>\n",
       "      <td>Her Majesty‚Äôs Coroner for the \\nNorthern Distr...</td>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11976 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  person  \\\n",
       "0                       malyun-karama-2020-0162_redacted   \n",
       "1                   theresa-robertson-2020-0158_redacted   \n",
       "2                     george-townsend-2020-0157_redacted   \n",
       "3                   jerrelle-mckenzie-2020-0144_redacted   \n",
       "4                       joan-williams-2020-0128_redacted   \n",
       "...                                                  ...   \n",
       "11971                    2014-0061-response-from-care-uk   \n",
       "11972                     neville-bardoliwalla-2020-0258   \n",
       "11973                            john-jennings-2020-0257   \n",
       "11974  sean-ennis-prevention-of-future-deaths-report-...   \n",
       "11975  barbara-mitchell-prevention-of-future-deaths-r...   \n",
       "\n",
       "                         date_of_report        ref  name_of_deceased  \\\n",
       "0        Date of report: 21 August 2020  2020-0162              5375   \n",
       "1         Date of report: 6 August 2020  2020-0158              5375   \n",
       "2           Date of report: 4 June 2020  2020-0157              5375   \n",
       "3          Date of report: 17 July 2020  2020-0144              5375   \n",
       "4          Date of report: 16 June 2020  2020-0128              5375   \n",
       "...                                 ...        ...               ...   \n",
       "11971                                    2014-0061              5375   \n",
       "11972  Date of report: 26 November 2020  2020-0258              5375   \n",
       "11973  Date of report: 26 November 2020  2020-0257              5375   \n",
       "11974  Date of report: 21 February 2022  2022-0054              5375   \n",
       "11975        Date of report: 12/05/2023  2023-0153              5375   \n",
       "\n",
       "      coroner_name coroner_area  \\\n",
       "0                                 \n",
       "1                                 \n",
       "2                                 \n",
       "3                                 \n",
       "4                                 \n",
       "...            ...          ...   \n",
       "11971                             \n",
       "11972                             \n",
       "11973                             \n",
       "11974                             \n",
       "11975                             \n",
       "\n",
       "                                                category  \\\n",
       "0      Category: Hospital death (Clinical procedure a...   \n",
       "1      Category: Alcohol, drug and medication related...   \n",
       "2      Category: Community healthcare related deaths,...   \n",
       "3      Category: Alcohol, drug and medication related...   \n",
       "4        Category: Road (Highways Safety) related deaths   \n",
       "...                                                  ...   \n",
       "11971                                                      \n",
       "11972            Category: Suicide; Other related deaths   \n",
       "11973  Category: Emergency services related deaths; O...   \n",
       "11974                     Category: Other related deaths   \n",
       "11975  Category: Care Home Health related deathsThis ...   \n",
       "\n",
       "                                                filename  \\\n",
       "0                   Malyun-Karama-2020-0162_Redacted.pdf   \n",
       "1               Theresa-Robertson-2020-0158_Redacted.pdf   \n",
       "2                 George-Townsend-2020-0157_Redacted.pdf   \n",
       "3               Jerrelle-McKenzie-2020-0144_Redacted.pdf   \n",
       "4                   Joan-Williams-2020-0128_Redacted.pdf   \n",
       "...                                                  ...   \n",
       "11971                2014-0061-Response-from-Care-UK.pdf   \n",
       "11972                 Neville-Bardoliwalla-2020-0258.pdf   \n",
       "11973                        John-Jennings-2020-0257.pdf   \n",
       "11974  Sean-Ennis-Prevention-of-future-deaths-report-...   \n",
       "11975  Barbara-Mitchell-Prevention-of-future-deaths-r...   \n",
       "\n",
       "                                                    text  \\\n",
       "0       \\n\\n \\n\\n \\n1 \\n\\n \\n2 \\n\\n \\n3 \\n\\n \\n4 \\n\\n...   \n",
       "1      Regulation 28:  Prevention of Future Deaths re...   \n",
       "2      REGULATION 28:  REPORT TO PREVENT FUTURE DEATH...   \n",
       "3      47812-2019 \\n\\nSenior Coroner - Emma Whitting ...   \n",
       "4      48060-2019\\n\\nSenior Coroner - Emma Whitting\\n...   \n",
       "...                                                  ...   \n",
       "11971  care  \\n\\nCare UK Clinical Services \\nLimited ...   \n",
       "11972   \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n ...   \n",
       "11973   \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n ...   \n",
       "11974  Her Majesty‚Äôs Coroner for the \\nNorthern Distr...   \n",
       "11975  Her Majesty‚Äôs Coroner for the \\nNorthern Distr...   \n",
       "\n",
       "                                                     url  url_ok  \n",
       "0      https://www.judiciary.uk/prevention-of-future-...    True  \n",
       "1      https://www.judiciary.uk/prevention-of-future-...    True  \n",
       "2      https://www.judiciary.uk/prevention-of-future-...    True  \n",
       "3      https://www.judiciary.uk/prevention-of-future-...    True  \n",
       "4      https://www.judiciary.uk/prevention-of-future-...    True  \n",
       "...                                                  ...     ...  \n",
       "11971  https://www.judiciary.uk/prevention-of-future-...    True  \n",
       "11972   https://www.judiciary.uk/?s=Neville+Bardoliwalla    True  \n",
       "11973  https://www.judiciary.uk/prevention-of-future-...    True  \n",
       "11974  https://www.judiciary.uk/prevention-of-future-...    True  \n",
       "11975  https://www.judiciary.uk/prevention-of-future-...    True  \n",
       "\n",
       "[11976 rows x 11 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4fcff05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the columns needed for Tantivy (order matters too if schema is sensitive)\n",
    "columns = [\n",
    "    \"person\", \"date_of_report\", \"ref\", \"name_of_deceased\", \"coroner_name\",\n",
    "    \"coroner_area\", \"category\", \"filename\", \"text\", \"url\"\n",
    "]\n",
    "df_tantivy = df[columns]\n",
    "\n",
    "# Save in newline-delimited JSON format (one JSON object per line)\n",
    "df_tantivy.to_json(\"data.json\", orient=\"records\", lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9567e0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"data.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7c516fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('elspeth_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
