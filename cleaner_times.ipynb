{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6b13154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a84ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('elspeth_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1be9d9e6-74ba-406e-978e-acac48059526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mass-casualty victims loaded:\n",
      "ref\n",
      "2017-0206    30\n",
      "2018-0262    71\n",
      "2019-0332     8\n",
      "Name: name_of_deceased, dtype: int64\n",
      "Wrote: pfd_onno_times_curated.csv\n",
      "Final ONOMAP rows: 5693\n",
      "Surname only retained: 4\n",
      "     Unique Identifier Forename    Surname\n",
      "708         5474860571      NaN     Pether\n",
      "2246        5711601195      NaN       Care\n",
      "3647        9241999471      NaN    Withers\n",
      "4116        1743138159      NaN  Van Tuyen\n",
      "Curated mass-casualty refs: ['2017-0206', '2018-0262', '2019-0332']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# FINAL FROZEN PIPELINE (curated mass-casualty refs)\n",
    "# ============================================================\n",
    "# Goal:\n",
    "# - Build ONOMAP-ready CSV with:\n",
    "#     Unique Identifier (10-digit), Forename, Surname\n",
    "# - Handle true mass-casualty events by extracting individual victim names\n",
    "#   from PDF text (Option A) and excluding event placeholder “names”.\n",
    "# - Keep surname-only individuals (e.g. \"Pether\") as Surname with blank Forename.\n",
    "#\n",
    "# Assumes df is your FULL dataframe with at least:\n",
    "#   - ref\n",
    "#   - name_of_deceased\n",
    "#   - text   (full extracted PDF text)\n",
    "#\n",
    "# Curated (manually verified) mass-casualty refs:\n",
    "#   - 2017-0206 (Sousse terror attack)\n",
    "#   - 2018-0262 (Grenfell Tower fire)\n",
    "#   - 2019-0332 (London Bridge terror attack)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Curated mass-casualty refs (manual truth set)\n",
    "# ----------------------------\n",
    "MASS_CASUALTY_REFS = {\n",
    "    \"2017-0206\",  # Sousse\n",
    "    \"2018-0262\",  # Grenfell Tower\n",
    "    \"2019-0332\",  # London Bridge\n",
    "}\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Curated victim lists for mass-casualty events (manual truth)\n",
    "# ----------------------------\n",
    "MASS_EVENT_VICTIMS = {\n",
    "    \"2017-0206\": [  # Sousse\n",
    "        \"Christopher Bell\",\n",
    "        \"Sharon Bell\",\n",
    "        \"Lisa Burbidge\",\n",
    "        \"Scott Chalkley\",\n",
    "        \"Stuart Cullen\",\n",
    "        \"Suzanne Davey\",\n",
    "        \"Christopher Dyer\",\n",
    "        \"Adrian Evans\",\n",
    "        \"Charles Evans\",\n",
    "        \"Angela Fisher\",\n",
    "        \"Raymond Fisher\",\n",
    "        \"Lisa Graham\",\n",
    "        \"William Graham\",\n",
    "        \"Philip Heathcote\",\n",
    "        \"Trudy Jones\",\n",
    "        \"Carly Lovett\",\n",
    "        \"Ann McQuire\",\n",
    "        \"James McQuire\",\n",
    "        \"Stephen Mellor\",\n",
    "        \"Joel Richards\",\n",
    "        \"John Stollery\",\n",
    "        \"Janet Stocker\",\n",
    "        \"John Stocker\",\n",
    "        \"Eileen Swannack\",\n",
    "        \"David Thompson\",\n",
    "        \"Denis Thwaites\",\n",
    "        \"Elaine Thwaites\",\n",
    "        \"John Welch\",\n",
    "        \"Bruce Wilkinson\",\n",
    "        \"Claire Windass\",\n",
    "    ],\n",
    "    \"2018-0262\": [ # Grenfell\n",
    "    \"Fatemeh Afraisabi\",\n",
    "    \"Sakina Afraisabi\",\n",
    "    \"Fathia Ahmed\",\n",
    "    \"Amal Ahmedin\",\n",
    "    \"Mohammad Al Haj Ali\",\n",
    "    \"Alexandra Atalla\",\n",
    "    \"Husnia Begum\",\n",
    "    \"Rabeya Begum\",\n",
    "    \"Leena Belkadi\",\n",
    "    \"Malak Belkadi\",\n",
    "    \"Omar Belkadi\",\n",
    "    \"Raymond Bernard\",\n",
    "    \"Vincent Chiejina\",\n",
    "    \"Fatima Choucair\",\n",
    "    \"Nadia Choucair\",\n",
    "    \"Siria Choucair\",\n",
    "    \"Bassem Choucair\",\n",
    "    \"Mierna Choucair\",\n",
    "    \"Zainab Choucair\",\n",
    "    \"Joseph Daniels\",\n",
    "    \"Jeremiah Deen\",\n",
    "    \"Zainab Deen\",\n",
    "    \"Anthony Disson\",\n",
    "    \"Eslah Elgwahry\",\n",
    "    \"Mariem Elgwahry\",\n",
    "    \"Abdulaziz El-Wahabi\",\n",
    "    \"Faouzia El-Wahabi\",\n",
    "    \"Mehdi El-Wahabi\",\n",
    "    \"Nur Huda El-Wahabi\",\n",
    "    \"Yasin El-Wahabi\",\n",
    "    \"Marco Gottardi\",\n",
    "    \"Berkat Haffar\",\n",
    "    \"Rinak Haffar\",\n",
    "    \"Farah Hamdan\",\n",
    "    \"Mohammed Hamid\",\n",
    "    \"Mohammed Hanif\",\n",
    "    \"Firdaws Hashim\",\n",
    "    \"Yaqub Hashim\",\n",
    "    \"Yahya Hashim\",\n",
    "    \"Fethia Hessen\",\n",
    "    \"Hania Hassan\",\n",
    "    \"Abrarles Mohamed Ibrahim\",\n",
    "    \"Ira Ibrahim\",\n",
    "    \"Rania Ibrahim\",\n",
    "    \"Amna Mahmud Idris\",\n",
    "    \"Ali Yawar Jafri\",\n",
    "    \"Nura Jemsil\",\n",
    "    \"Hamid Kani\",\n",
    "    \"Hashim Kedir\",\n",
    "    \"Khadija Khalloufi\",\n",
    "    \"Victoria King\",\n",
    "    \"Deborah Lamprell\",\n",
    "    \"Gary Maunders\",\n",
    "    \"Mary Mendy\",\n",
    "    \"Karru Miah\",\n",
    "    \"Liqoya Moore\",\n",
    "    \"Dennis Murphy\",\n",
    "    \"Mohammed Amied Neda\",\n",
    "    \"Isaac Paulos\",\n",
    "    \"Steven Power\",\n",
    "    \"Hesham Rahman\",\n",
    "    \"Khadija Saye\",\n",
    "    \"Abdeslam Sebbar\",\n",
    "    \"Sheila Smith\",\n",
    "    \"Gloria Trevisan\",\n",
    "    \"Amaya Tuocu-Ahmedin\",\n",
    "    \"Mohamednur Tuczu\",\n",
    "    \"Jessica Urbano-Ramirez\",\n",
    "    \"Marjorie Vital\",\n",
    "    \"Ernie Vital\",\n",
    "    \"Baby Logan Gomes\",\n",
    "    ],\n",
    "    \"2019-0332\": [  # London Bridge\n",
    "        \"Xavier Thomas\",\n",
    "        \"Christine Archibald\",\n",
    "        \"James McMullan\",\n",
    "        \"Alexandre Pigeard\",\n",
    "        \"Kirsty Boden\",\n",
    "        \"Sébastien Bélanger\",\n",
    "        \"Sara Zelenak\",\n",
    "        \"Ignacio Echeverria Miralles De Imperial\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Build mass-casualty people dataframe from curated lists\n",
    "# ----------------------------\n",
    "event_people = []\n",
    "\n",
    "for ref, names in MASS_EVENT_VICTIMS.items():\n",
    "    for name in names:\n",
    "        event_people.append({\n",
    "            \"ref\": ref,\n",
    "            \"name_of_deceased\": name\n",
    "        })\n",
    "\n",
    "mass_event_people_df = (\n",
    "    pd.DataFrame(event_people)\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"Mass-casualty victims loaded:\")\n",
    "print(mass_event_people_df.groupby(\"ref\")[\"name_of_deceased\"].count())\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Build combined names dataframe:\n",
    "#    - non-event refs: use df['name_of_deceased']\n",
    "#    - curated event refs: use extracted victim list (NOT event placeholder)\n",
    "# ----------------------------\n",
    "df_non_event_names = df.loc[\n",
    "    ~df[\"ref\"].astype(str).isin(MASS_CASUALTY_REFS),\n",
    "    [\"ref\", \"name_of_deceased\"]\n",
    "].copy()\n",
    "\n",
    "names_df = pd.concat([df_non_event_names, mass_event_people_df], ignore_index=True)\n",
    "\n",
    "# Drop blanks early\n",
    "names_df = names_df[\n",
    "    names_df[\"name_of_deceased\"].notna() &\n",
    "    (names_df[\"name_of_deceased\"].astype(str).str.strip() != \"\")\n",
    "].copy()\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Shared-surname expansion (handles and/& and comma-lists)\n",
    "# ----------------------------\n",
    "JOINER = re.compile(r'(?i)\\b(?:and)\\b|&')\n",
    "\n",
    "def expand_shared_surname(x) -> str:\n",
    "    if x is None:\n",
    "        return x\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return s\n",
    "\n",
    "    # normalise unicode punctuation\n",
    "    s = (s.replace(\"\\u2018\", \"'\").replace(\"\\u2019\", \"'\")\n",
    "           .replace(\"\\u2013\", \"-\").replace(\"\\u2014\", \"-\"))\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    if not JOINER.search(s):\n",
    "        return s\n",
    "\n",
    "    # normalise sluggy joiners near and/& (e.g. Bryan-and-Mary-Andrews)\n",
    "    s_near = re.sub(r'(?i)\\s*[-_]+\\s*(and|&)\\s*[-_]+\\s*', r' \\1 ', s)\n",
    "\n",
    "    # Case: \"A, B and C Surname\" or \"A, B & C Surname\"\n",
    "    m_list = re.match(\n",
    "        r\"^\\s*(?P<list>(?:[A-Za-zÀ-ÖØ-öø-ÿ'’\\-\\.]+\\s*,\\s*)+)\"\n",
    "        r\"(?P<last>[A-Za-zÀ-ÖØ-öø-ÿ'’\\-\\.]+)\\s+(?:and|&)\\s+\"\n",
    "        r\"(?P<final>[A-Za-zÀ-ÖØ-öø-ÿ'’\\-\\.]+)\\s+\"\n",
    "        r\"(?P<surname>.+?)\\s*$\",\n",
    "        s_near\n",
    "    )\n",
    "    if m_list:\n",
    "        list_part = m_list.group(\"list\")\n",
    "        last_in_list = m_list.group(\"last\")\n",
    "        final_forename = m_list.group(\"final\")\n",
    "        surname = m_list.group(\"surname\").strip()\n",
    "\n",
    "        # guardrails\n",
    "        if re.search(r\"\\d\", surname):\n",
    "            return s\n",
    "        if not re.match(r\"^[A-Za-zÀ-ÖØ-öø-ÿ'’\\-\\. ]+$\", surname):\n",
    "            return s\n",
    "\n",
    "        forenames = [p.strip() for p in list_part.split(\",\") if p.strip()]\n",
    "        forenames.append(last_in_list.strip())\n",
    "        forenames.append(final_forename.strip())\n",
    "        return \", \".join([f\"{fn} {surname}\" for fn in forenames])\n",
    "\n",
    "    # Case: \"A and B Surname\" or \"A & B Surname\"\n",
    "    m_two = re.match(\n",
    "        r\"^\\s*([A-Za-zÀ-ÖØ-öø-ÿ'’\\-\\.]+)\\s+(?:and|&)\\s+([A-Za-zÀ-ÖØ-öø-ÿ'’\\-\\.]+)\\s+(.+?)\\s*$\",\n",
    "        s_near\n",
    "    )\n",
    "    if not m_two:\n",
    "        return s\n",
    "\n",
    "    a, b, surname = m_two.group(1).strip(), m_two.group(2).strip(), m_two.group(3).strip()\n",
    "    if re.search(r\"\\d\", surname):\n",
    "        return s\n",
    "    if not re.match(r\"^[A-Za-zÀ-ÖØ-öø-ÿ'’\\-\\. ]+$\", surname):\n",
    "        return s\n",
    "    return f\"{a} {surname}, {b} {surname}\"\n",
    "\n",
    "names_df[\"name_of_deceased\"] = names_df[\"name_of_deceased\"].map(expand_shared_surname)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Clean + split/explode (single pass)\n",
    "# ----------------------------\n",
    "SPLIT_PAT = r\"(?x),|;|\\s+&\\s+|\\s+and\\s+|(?:\\s*[-–—_]\\s*and\\s*[-–—_]\\s*)\"\n",
    "\n",
    "# Existing cut (requires separators/colons)\n",
    "CUT_PAT = re.compile(r\"\"\"(?ix)\n",
    "    (?:\\s*(?:\\||\\n| - | — | – )\\s*|\n",
    "       \\s*\\b(?:coroner\\s+name|coroner\\s+area|category|sent\\s+to|to|recipient)\\s*:\\s*\n",
    "    )\n",
    "    (?:coroner\\s+name|coroner\\s+area|category|this\\s+report|regulation\\s*28|\n",
    "       report\\s+is\\s+being\\s+sent|sent\\s+to|recipient|to|nhs\\s+trust|\n",
    "       department\\s+of\\s+health|integrated\\s+care|commission|council)\\b\n",
    "    .*$\"\"\")\n",
    "\n",
    "# NEW: hard-cut even when metadata is glued to the surname (e.g. \"WochnaCoroner name:\")\n",
    "META_CUT = re.compile(r\"\"\"(?ix)\n",
    "    \\b(\n",
    "        coroner\\s+name\\s*:|\n",
    "        coroner\\s+area\\s*:|\n",
    "        category\\s*:|\n",
    "        this\\s+report\\s+is\\s+being\\s+sent\\s+to\\s*:|\n",
    "        report\\s+is\\s+being\\s+sent\\s+to\\s*:|\n",
    "        sent\\s+to\\s*:|\n",
    "        recipient\\s*:\n",
    "    ).*$\n",
    "\"\"\")\n",
    "\n",
    "names_df = (\n",
    "    names_df.assign(\n",
    "        name_of_deceased=(\n",
    "            names_df[\"name_of_deceased\"]\n",
    "            .astype(str)\n",
    "\n",
    "            # Unicode punctuation normalisation\n",
    "            .str.replace(\"\\u2018\", \"'\", regex=False)\n",
    "            .str.replace(\"\\u2019\", \"'\", regex=False)\n",
    "            .str.replace(\"\\u2013\", \"-\", regex=False)\n",
    "            .str.replace(\"\\u2014\", \"-\", regex=False)\n",
    "\n",
    "            # ✅ Move CamelCase deglue EARLY (so \"WochnaCoroner\" becomes \"Wochna Coroner\")\n",
    "            .str.replace(r\"(?<=[a-z])(?=[A-Z])\", \" \", regex=True)\n",
    "\n",
    "            # ✅ Hard-cut at metadata keywords (even if glued / no separators)\n",
    "            .str.replace(META_CUT, \"\", regex=True)\n",
    "\n",
    "            # Existing leading label removal\n",
    "            .str.replace(r\"(?i)^\\s*(deceased|deceased names?)\\s*[:\\-]\\s*\", \"\", regex=True)\n",
    "\n",
    "            # Existing cut (separator-based)\n",
    "            .str.replace(CUT_PAT, \"\", regex=True)\n",
    "\n",
    "            # Remove bracketed notes + titles\n",
    "            .str.replace(r\"\\(.*?\\)\", \"\", regex=True)\n",
    "            .str.replace(r\"\\[.*?\\]\", \"\", regex=True)\n",
    "            .str.replace(r\"(?i)\\b(mr|mrs|ms|miss|dr|prof|sir|madam)\\b\\.?\", \"\", regex=True)\n",
    "            .str.replace(r\"(?i)\\b(the\\s+late|late)\\b\", \"\", regex=True)\n",
    "\n",
    "            # Whitespace tidy\n",
    "            .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "            .str.strip()\n",
    "\n",
    "            # Split multiple names\n",
    "            .str.split(SPLIT_PAT, regex=True)\n",
    "        )\n",
    "    )\n",
    "    .explode(\"name_of_deceased\")\n",
    "    .assign(\n",
    "        name_of_deceased=lambda d: (\n",
    "            d[\"name_of_deceased\"]\n",
    "            .astype(str)\n",
    "            .str.replace(r\"^[\\s,;:.|\\-–—]+|[\\s,;:.|\\-–—]+$\", \"\", regex=True)\n",
    "            .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "            .str.strip()\n",
    "        )\n",
    "    )\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Drop obvious placeholders\n",
    "names_df = names_df[\n",
    "    names_df[\"name_of_deceased\"].notna()\n",
    "    & (names_df[\"name_of_deceased\"].astype(str).str.strip() != \"\")\n",
    "    & (~names_df[\"name_of_deceased\"].str.match(r\"(?i)^(unknown|not known|n/?a|unidentified)$\", na=False))\n",
    "].copy()\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Split Forename/Surname\n",
    "#    IMPORTANT: single token -> Surname (keeps Pether etc. for ONOMAP)\n",
    "# ----------------------------\n",
    "SURNAME_PARTICLES = {\n",
    "    \"al\", \"el\",\n",
    "    \"da\", \"de\", \"del\", \"della\", \"der\", \"den\",\n",
    "    \"di\", \"du\",\n",
    "    \"la\", \"le\", \"lo\",\n",
    "    \"van\", \"von\",\n",
    "    \"bin\", \"ibn\",\n",
    "    \"st\", \"st.\", \"saint\",\n",
    "    \"ter\",\n",
    "}\n",
    "\n",
    "def split_forename_surname(full: str):\n",
    "    if full is None:\n",
    "        return (np.nan, np.nan)\n",
    "    s = str(full).strip()\n",
    "    if not s:\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    parts = s.split()\n",
    "    if len(parts) == 1:\n",
    "        return (np.nan, parts[0])  # surname-only\n",
    "\n",
    "    i = len(parts) - 1\n",
    "    surname_parts = [parts[i]]\n",
    "    i -= 1\n",
    "    while i >= 0 and parts[i].rstrip(\".\").lower() in SURNAME_PARTICLES:\n",
    "        surname_parts.insert(0, parts[i])\n",
    "        i -= 1\n",
    "\n",
    "    surname = \" \".join(surname_parts)\n",
    "    forename = \" \".join(parts[: i + 1]).strip() or np.nan\n",
    "    return (forename, surname)\n",
    "\n",
    "spl = names_df[\"name_of_deceased\"].apply(split_forename_surname)\n",
    "names_df[\"Forename\"] = spl.map(lambda x: x[0])\n",
    "names_df[\"Surname\"] = spl.map(lambda x: x[1])\n",
    "\n",
    "# Require surname for ONOMAP\n",
    "names_df = names_df.dropna(subset=[\"Surname\"]).copy()\n",
    "names_df[\"surname_only\"] = names_df[\"Forename\"].isna() & names_df[\"Surname\"].notna()\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Deduplicate per (ref, Forename, Surname)\n",
    "# ----------------------------\n",
    "df_dedup = (\n",
    "    names_df\n",
    "    .drop_duplicates(subset=[\"ref\", \"Forename\", \"Surname\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Stable 10-digit Unique Identifier\n",
    "# ----------------------------\n",
    "def make_onomap_id(row) -> str:\n",
    "    forename = \"\" if pd.isna(row[\"Forename\"]) else row[\"Forename\"]\n",
    "    key = f\"{row['ref']}|{forename}|{row['Surname']}\"\n",
    "    h = hashlib.sha256(key.encode(\"utf-8\")).hexdigest()\n",
    "    return str(int(h, 16))[:10]\n",
    "\n",
    "df_dedup[\"Unique Identifier\"] = df_dedup.apply(make_onomap_id, axis=1)\n",
    "\n",
    "assert df_dedup[\"Unique Identifier\"].is_unique, \"Duplicate Unique Identifiers found\"\n",
    "assert df_dedup[\"Unique Identifier\"].astype(str).str.match(r\"^\\d{10}$\").all(), \"ID format error\"\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Final exclusions (auditable, minimal)\n",
    "# ----------------------------\n",
    "# Rationale:\n",
    "# - 'Sousse' is an event/location label, not an individual; we instead use victim names extracted from PDF text\n",
    "# - 'Tower' is the Grenfell Tower event placeholder (Forename='Grenfell', Surname='Tower'); victims extracted from PDF text\n",
    "# - 'Redacted', 'C', 'Yz' are administrative/non-informative tokens for ethnicity inference\n",
    "# - 'M5', refers to M5 '7' https://www.judiciary.uk/wp-content/uploads/2015/01/M5-Seven-2014-0564.pdf - doesn't name the 7\n",
    "exclude_surnames = {\"Sousse\", \"Redacted\", \"REDACTED\", \"C\", \"Yz\", \"YZ\", \"Tower\", \"M5\"}\n",
    "\n",
    "df_dedup = df_dedup[~df_dedup[\"Surname\"].isin(exclude_surnames)].copy()\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Export ONOMAP input\n",
    "# ----------------------------\n",
    "onomap_df = df_dedup[[\"Unique Identifier\", \"Forename\", \"Surname\"]].copy()\n",
    "onomap_df[\"Unique Identifier\"] = onomap_df[\"Unique Identifier\"].astype(str)\n",
    "\n",
    "onomap_df.to_csv(\n",
    "    \"pfd_onno_times_curated.csv\",\n",
    "    index=False,\n",
    "    quoting=csv.QUOTE_NONNUMERIC\n",
    ")\n",
    "\n",
    "print(\"Wrote: pfd_onno_times_curated.csv\")\n",
    "print(\"Final ONOMAP rows:\", len(onomap_df))\n",
    "surname_only = onomap_df[onomap_df[\"Forename\"].isna() | (onomap_df[\"Forename\"].astype(str).str.strip() == \"\")]\n",
    "print(\"Surname only retained:\", len(surname_only))\n",
    "print(surname_only.head(30))\n",
    "print(\"Curated mass-casualty refs:\", sorted(MASS_CASUALTY_REFS))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
