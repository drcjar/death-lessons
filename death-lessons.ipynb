{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get the pdf urls for preventing future death reports (and their responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "people_urls = []\n",
    "pagenum = 0\n",
    "URL = 'https://www.judiciary.uk/publication-type/pfd-report/page/' + str(pagenum) + '/'\n",
    "page = requests.get(URL)\n",
    "\n",
    "while (page.status_code != 404):\n",
    "    URL = 'https://www.judiciary.uk/publication-type/pfd-report/page/' + str(pagenum) + '/'\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    results = soup.find(id='main-content')\n",
    "    people = results.find_all('h5', class_='entry-title')\n",
    "    people = [people[i].find('a')['href'] for i in range(0,len(people))] # target urls\n",
    "    people_urls.append(people)\n",
    "    pagenum += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_urls\n",
    "flat_list = [item for sublist in people_urls for item in sublist]\n",
    "targets = pd.DataFrame(flat_list)\n",
    "targets.columns = ['urls']\n",
    "targets['urls'].drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = list(targets['urls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_urls = []\n",
    "\n",
    "for URL in urls:\n",
    "    print(URL)\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    for link in soup.find_all('a'):\n",
    "        current_link = link.get('href')\n",
    "        if current_link.endswith('pdf'):  \n",
    "            pdf_urls.append(current_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pdf_urls).to_csv('pdf_urls.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(urls).to_csv('urls.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
